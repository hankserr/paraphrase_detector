{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMk1B5jqp51P9RV6PhlThpZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hankserr/paraphrase_detector/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9FUcQ5VbxfMo",
        "outputId": "fef43baf-8dac-4b49-835a-a8740bd1ab55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "1\n",
            "20000\n",
            "\n",
            "Train: 16000 \n",
            "\n",
            "\n",
            "Test: 4000 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFDebertaV2ForSequenceClassification.\n",
            "\n",
            "Some layers of TFDebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "1000/1000 [==============================] - 387s 349ms/step - loss: 0.0899 - accuracy: 0.9945 - val_loss: 0.0280 - val_accuracy: 0.9945\n",
            "Epoch 2/6\n",
            "1000/1000 [==============================] - 336s 336ms/step - loss: 0.0162 - accuracy: 0.9968 - val_loss: 0.0089 - val_accuracy: 0.9967\n",
            "Epoch 3/6\n",
            "1000/1000 [==============================] - 359s 359ms/step - loss: 0.0060 - accuracy: 0.9988 - val_loss: 0.0031 - val_accuracy: 0.9987\n",
            "Epoch 4/6\n",
            "1000/1000 [==============================] - 359s 359ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 0.9998\n",
            "Epoch 5/6\n",
            "1000/1000 [==============================] - 356s 356ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 0.9998\n",
            "Epoch 6/6\n",
            "1000/1000 [==============================] - 358s 358ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 0.9998\n",
            "Diff\n",
            "Predicted class: 1 (0 means not paraphrased, 1 means paraphrased)\n",
            "Probabilities: [0.00250539 0.9974946 ]\n",
            "Same\n",
            "Predicted class: 1 (0 means not paraphrased, 1 means paraphrased)\n",
            "Probabilities: [2.2308688e-05 9.9997771e-01]\n",
            "Reorder\n",
            "Predicted class: 1 (0 means not paraphrased, 1 means paraphrased)\n",
            "Probabilities: [4.984133e-05 9.999502e-01]\n",
            "Para\n",
            "Predicted class: 0 (0 means not paraphrased, 1 means paraphrased)\n",
            "Probabilities: [0.8426618  0.15733825]\n"
          ]
        }
      ],
      "source": [
        "# Version: 1.0.2 deberta-v3-small dates dataset\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "\n",
        "# Import required libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    TFAutoModelForSequenceClassification, AutoTokenizer,\n",
        "    DataCollatorWithPadding, create_optimizer,\n",
        "    DataCollatorForSeq2Seq, TFAutoModelForSeq2SeqLM\n",
        ")\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "def load_datasets():\n",
        "    \"\"\"Load the PAWS dataset.\"\"\"\n",
        "    # train_dataset = load_dataset(\"paws\", \"labeled_final\", split=\"train\")\n",
        "    # test_dataset = load_dataset(\"paws\", \"labeled_final\", split=\"test\")\n",
        "    \"\"\"Load the paraphrase_dataset_dates dataset.\"\"\"\n",
        "    dataset = load_dataset('csv', data_files=['paraphrase_dataset_dates.csv'])\n",
        "    dataset = dataset[\"train\"]\n",
        "    # Example: Using a 80-20 split for train-test\n",
        "    split_ratio = 0.8\n",
        "    split_point = int(split_ratio * len(dataset))\n",
        "\n",
        "    train_dataset = dataset.select(range(split_point))  # First 80% for training\n",
        "    test_dataset = dataset.select(range(split_point, len(dataset)))  # Remaining 20% for testing\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def preprocess_datasets(train_dataset, test_dataset, tokenizer):\n",
        "    \"\"\"Preprocess the datasets by tokenizing and formatting.\"\"\"\n",
        "    def encode(examples):\n",
        "      # Need to replace sentence1/sentence2 with Input & Paraphrase\n",
        "      return tokenizer(examples[\"Input\"], examples[\"Paraphrase\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    train_dataset = train_dataset.map(encode, batched=True)\n",
        "    test_dataset = test_dataset.map(encode, batched=True)\n",
        "\n",
        "    # Needed to switch from labels to Type\n",
        "    train_dataset = train_dataset.map(lambda examples: {\"labels\": examples[\"Type\"]})\n",
        "    test_dataset = test_dataset.map(lambda examples: {\"labels\": examples[\"Type\"]})\n",
        "\n",
        "    train_dataset.set_format(type=\"tensorflow\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    test_dataset.set_format(type=\"tensorflow\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def create_data_collator(tokenizer):\n",
        "    \"\"\"Create a data collator for dynamic padding.\"\"\"\n",
        "    return DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Load the DistilBERT model.\"\"\"\n",
        "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "    return TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"microsoft/deberta-v3-small\", num_labels=2\n",
        "    )\n",
        "\n",
        "def create_optimizer_and_schedule(dataset, batch_size=16, num_epochs=4):\n",
        "    \"\"\"Create an optimizer and learning rate schedule.\"\"\"\n",
        "    batches_per_epoch = len(dataset) // batch_size\n",
        "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "    return create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    # decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "def prepare_tf_datasets(model, train_dataset, test_dataset, data_collator, batch_size=16):\n",
        "    \"\"\"Convert datasets to TensorFlow datasets.\"\"\"\n",
        "    tf_train_set = model.prepare_tf_dataset(\n",
        "        train_dataset,\n",
        "        shuffle=True,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    tf_validation_set = model.prepare_tf_dataset(\n",
        "        test_dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    return tf_train_set, tf_validation_set\n",
        "\n",
        "def train_model(model, tf_train_set, tf_validation_set, optimizer, num_epochs=6):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    model.compile(optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "    metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath='model_checkpoint_epoch_{epoch:02d}.h5',\n",
        "        save_weights_only=True,\n",
        "        save_freq='epoch',\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x=tf_train_set,\n",
        "        validation_data=tf_validation_set,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=[metric_callback, checkpoint_callback]\n",
        "    )\n",
        "\n",
        "def predict_paraphrase(sentence1, sentence2, tokenizer, model):\n",
        "    inputs = tokenizer(sentence1, sentence2, return_tensors='tf', truncation=True, padding='max_length', max_length=128)\n",
        "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "    logits = outputs.logits\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    predicted_class = tf.argmax(logits, axis=-1).numpy()[0]\n",
        "\n",
        "    return predicted_class, probabilities.numpy()[0]\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_dataset, test_dataset = load_datasets()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n",
        "    train_dataset, test_dataset = preprocess_datasets(train_dataset, test_dataset, tokenizer)\n",
        "    data_collator = create_data_collator(tokenizer)\n",
        "    model = load_model()\n",
        "    optimizer, _ = create_optimizer_and_schedule(train_dataset)\n",
        "    tf_train_set, tf_validation_set = prepare_tf_datasets(model, train_dataset, test_dataset, data_collator)\n",
        "    train_model(model, tf_train_set, tf_validation_set, optimizer)\n",
        "    diff_1 = \"02/23/2025\"\n",
        "    diff_2 = \"January 23, 2025\"\n",
        "\n",
        "    same_1 = \"January 23, 2024\"\n",
        "    same_2 = \"Jan 23, 24\"\n",
        "\n",
        "    reorder_1 = \"January 23, 2024\"\n",
        "    reorder_2 = \"01/23/24\"\n",
        "\n",
        "    para_1 = \"5-10-99\"\n",
        "    para_2 = \"Mar 10 1999\"\n",
        "\n",
        "    # Predict if the sentences are paraphrases\n",
        "    predicted_class, probabilities = predict_paraphrase(diff_1, diff_2, tokenizer, model)\n",
        "    print(\"Diff\")\n",
        "    print(f\"Predicted class: {predicted_class} (0 means not paraphrased, 1 means paraphrased)\")\n",
        "    print(f\"Probabilities: {probabilities}\")\n",
        "\n",
        "    predicted_class, probabilities = predict_paraphrase(same_1, same_2, tokenizer, model)\n",
        "    print(\"Same\")\n",
        "    print(f\"Predicted class: {predicted_class} (0 means not paraphrased, 1 means paraphrased)\")\n",
        "    print(f\"Probabilities: {probabilities}\")\n",
        "\n",
        "    predicted_class, probabilities = predict_paraphrase(reorder_1, reorder_2, tokenizer, model)\n",
        "    print(\"Reorder\")\n",
        "    print(f\"Predicted class: {predicted_class} (0 means not paraphrased, 1 means paraphrased)\")\n",
        "    print(f\"Probabilities: {probabilities}\")\n",
        "\n",
        "    predicted_class, probabilities = predict_paraphrase(para_1, para_2, tokenizer, model)\n",
        "    print(\"Para\")\n",
        "    print(f\"Predicted class: {predicted_class} (0 means not paraphrased, 1 means paraphrased)\")\n",
        "    print(f\"Probabilities: {probabilities}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}